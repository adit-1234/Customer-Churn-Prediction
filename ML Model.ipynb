{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (100000, 7)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 7 columns):\n",
      " #   Column                      Non-Null Count   Dtype  \n",
      "---  ------                      --------------   -----  \n",
      " 0   Age                         100000 non-null  int64  \n",
      " 1   Gender                      100000 non-null  object \n",
      " 2   Location                    100000 non-null  object \n",
      " 3   Subscription_Length_Months  100000 non-null  int64  \n",
      " 4   Monthly_Bill                100000 non-null  float64\n",
      " 5   Total_Usage_GB              100000 non-null  int64  \n",
      " 6   Churn                       100000 non-null  int64  \n",
      "dtypes: float64(1), int64(4), object(2)\n",
      "memory usage: 5.3+ MB\n",
      "None\n",
      "   Age  Gender     Location  Subscription_Length_Months  Monthly_Bill  \\\n",
      "0   63    Male  Los Angeles                          17         73.36   \n",
      "1   62  Female     New York                           1         48.76   \n",
      "2   24  Female  Los Angeles                           5         85.47   \n",
      "3   36  Female        Miami                           3         97.94   \n",
      "4   46  Female        Miami                          19         58.14   \n",
      "\n",
      "   Total_Usage_GB  Churn  \n",
      "0             236      0  \n",
      "1             172      0  \n",
      "2             460      0  \n",
      "3             297      1  \n",
      "4             266      0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the dataset into a Pandas DataFrame\n",
    "data = pd.read_csv('customer_churn_large_dataset.csv')  \n",
    "\n",
    "# Remove columns that are not relevant for modeling\n",
    "data.drop(['CustomerID', 'Name'], axis=1, inplace=True)\n",
    "\n",
    "# Initial Exploration\n",
    "print(\"Dataset shape:\", data.shape)\n",
    "print(data.info())\n",
    "print(data.head())\n",
    "\n",
    "# Handle Missing Data\n",
    "imputer = SimpleImputer(strategy='mean')  \n",
    "data['Monthly_Bill'] = imputer.fit_transform(data['Monthly_Bill'].values.reshape(-1, 1))\n",
    "\n",
    "# Encode Categorical Variables (One-Hot Encoding)\n",
    "data = pd.get_dummies(data, columns=['Gender', 'Location'], drop_first=True)\n",
    "\n",
    "# Split the data into features and target variable\n",
    "X = data.drop('Churn', axis=1)\n",
    "y = data['Churn']\n",
    "\n",
    "# Split the dataset into training and testing sets (e.g., 80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Scaling (Standardization)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age  Monthly_Bill  Churn  Gender_Male  Location_Houston  \\\n",
      "0   63         73.36      0         True             False   \n",
      "1   62         48.76      0        False             False   \n",
      "2   24         85.47      0        False             False   \n",
      "3   36         97.94      1        False             False   \n",
      "4   46         58.14      0        False             False   \n",
      "\n",
      "   Location_Los Angeles  Location_Miami  Location_New York  CustomerTenure  \\\n",
      "0                  True           False              False        0.058824   \n",
      "1                 False           False               True        1.000000   \n",
      "2                  True           False              False        0.200000   \n",
      "3                 False            True              False        0.333333   \n",
      "4                 False            True              False        0.052632   \n",
      "\n",
      "   Average_Usage_Per_Month  \n",
      "0                13.882353  \n",
      "1               172.000000  \n",
      "2                92.000000  \n",
      "3                99.000000  \n",
      "4                14.000000  \n"
     ]
    }
   ],
   "source": [
    "# Calculate Customer Tenure\n",
    "# You can create a new feature representing the tenure of each customer, which is the inverse of Subscription_Length_Months.\n",
    "data['CustomerTenure'] = 1 / data['Subscription_Length_Months']\n",
    "\n",
    "# Calculate Average Usage Per Month\n",
    "# Create a new feature representing the average monthly data usage per customer.\n",
    "data['Average_Usage_Per_Month'] = data['Total_Usage_GB'] / data['Subscription_Length_Months']\n",
    "\n",
    "# Drop the original 'Subscription_Length_Months' and 'Total_Usage_GB' columns since we have derived new features\n",
    "data.drop(['Subscription_Length_Months', 'Total_Usage_GB'], axis=1, inplace=True)\n",
    "\n",
    "# Now, the dataset includes the new features 'CustomerTenure' and 'Average_Usage_Per_Month'\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.50\n",
      "Precision: 0.50\n",
      "Recall: 0.38\n",
      "F1 Score: 0.43\n",
      "ROC AUC: 0.50\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "data = pd.read_csv('customer_churn_large_dataset.csv')  \n",
    "# Remove columns that are not relevant for modeling\n",
    "data.drop(['CustomerID', 'Name'], axis=1, inplace=True)\n",
    "\n",
    "# Split the data into features (X) and the target variable (y)\n",
    "X = data.drop('Churn', axis=1)\n",
    "y = data['Churn']\n",
    "\n",
    "# Encode Categorical Variables (One-Hot Encoding)\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "categorical_cols = ['Gender', 'Location']  \n",
    "X_encoded = encoder.fit_transform(X[categorical_cols])\n",
    "\n",
    "# Replace the original categorical columns with the one-hot encoded versions\n",
    "X = X.drop(categorical_cols, axis=1)\n",
    "X = pd.concat([X, pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(categorical_cols))], axis=1)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Scaling (Standardization)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Model Building: Logistic Regression (You can replace this with other models)\n",
    "model = LogisticRegression(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "# Print model evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.2f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.49\n",
      "Precision: 0.49\n",
      "Recall: 0.47\n",
      "F1 Score: 0.48\n",
      "ROC AUC: 0.49\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "data = pd.read_csv('customer_churn_large_dataset.csv') \n",
    "# Remove columns that are not relevant for modeling\n",
    "data.drop(['CustomerID', 'Name'], axis=1, inplace=True)\n",
    "\n",
    "# Split the data into features (X) and the target variable (y)\n",
    "X = data.drop('Churn', axis=1)\n",
    "y = data['Churn']\n",
    "\n",
    "# Encode Categorical Variables (One-Hot Encoding)\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "categorical_cols = ['Gender', 'Location']  \n",
    "X_encoded = encoder.fit_transform(X[categorical_cols])\n",
    "\n",
    "# Replace the original categorical columns with the one-hot encoded versions\n",
    "X = X.drop(categorical_cols, axis=1)\n",
    "X = pd.concat([X, pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(categorical_cols))], axis=1)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Scaling (Standardization)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Model Building: Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, rf_model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "# Print model evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.50\n",
      "Precision: 0.50\n",
      "Recall: 0.45\n",
      "F1 Score: 0.47\n",
      "ROC AUC: 0.50\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "data = pd.read_csv('customer_churn_large_dataset.csv')  \n",
    "# Remove columns that are not relevant for modeling\n",
    "data.drop(['CustomerID', 'Name'], axis=1, inplace=True)\n",
    "# Split the data into features (X) and the target variable (y)\n",
    "X = data.drop('Churn', axis=1)\n",
    "y = data['Churn']\n",
    "\n",
    "# Encode Categorical Variables (One-Hot Encoding)\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "categorical_cols = ['Gender', 'Location'] \n",
    "X_encoded = encoder.fit_transform(X[categorical_cols])\n",
    "\n",
    "# Replace the original categorical columns with the one-hot encoded versions\n",
    "X = X.drop(categorical_cols, axis=1)\n",
    "X = pd.concat([X, pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(categorical_cols))], axis=1)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Scaling (Standardization)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Model Building: Gradient Boosting Classifier\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, gb_model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "# Print model evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Download\\PYTHON\\Lib\\site-packages\\xgboost\\data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.50\n",
      "Precision: 0.50\n",
      "Recall: 0.49\n",
      "F1 Score: 0.49\n",
      "ROC AUC: 0.50\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "data = pd.read_csv('customer_churn_large_dataset.csv') \n",
    "# Remove columns that are not relevant for modeling\n",
    "data.drop(['CustomerID', 'Name'], axis=1, inplace=True)\n",
    "\n",
    "# Split the data into features (X) and the target variable (y)\n",
    "X = data.drop('Churn', axis=1)\n",
    "y = data['Churn']\n",
    "\n",
    "# Encode Categorical Variables (One-Hot Encoding)\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "categorical_cols = ['Gender', 'Location']  \n",
    "X_encoded = encoder.fit_transform(X[categorical_cols])\n",
    "\n",
    "# Replace the original categorical columns with the one-hot encoded versions\n",
    "X = X.drop(categorical_cols, axis=1)\n",
    "X = pd.concat([X, pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(categorical_cols))], axis=1)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Scaling (Standardization)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Model Building: XGBoost Classifier\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, xgb_model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "# Print model evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1250/1250 [==============================] - 3s 1ms/step - loss: 0.6943 - accuracy: 0.5023\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6935 - accuracy: 0.5034\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6932 - accuracy: 0.5052\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6929 - accuracy: 0.5100\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6930 - accuracy: 0.5094\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6929 - accuracy: 0.5101\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6928 - accuracy: 0.5110\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6928 - accuracy: 0.5116\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6928 - accuracy: 0.5129\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6927 - accuracy: 0.5121\n",
      "Epoch 11/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6927 - accuracy: 0.5135\n",
      "Epoch 12/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6926 - accuracy: 0.5123\n",
      "Epoch 13/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6925 - accuracy: 0.5135\n",
      "Epoch 14/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6925 - accuracy: 0.5139\n",
      "Epoch 15/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6924 - accuracy: 0.5176\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6923 - accuracy: 0.5159\n",
      "Epoch 17/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6922 - accuracy: 0.5170\n",
      "Epoch 18/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6922 - accuracy: 0.5200\n",
      "Epoch 19/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6922 - accuracy: 0.5171\n",
      "Epoch 20/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6920 - accuracy: 0.5191\n",
      "Epoch 21/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6919 - accuracy: 0.5188\n",
      "Epoch 22/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6919 - accuracy: 0.5217\n",
      "Epoch 23/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6918 - accuracy: 0.5211\n",
      "Epoch 24/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6917 - accuracy: 0.5230\n",
      "Epoch 25/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6916 - accuracy: 0.5228\n",
      "Epoch 26/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6915 - accuracy: 0.5220\n",
      "Epoch 27/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6915 - accuracy: 0.5208\n",
      "Epoch 28/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6913 - accuracy: 0.5230\n",
      "Epoch 29/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6913 - accuracy: 0.5224\n",
      "Epoch 30/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6913 - accuracy: 0.5236\n",
      "Epoch 31/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6912 - accuracy: 0.5231\n",
      "Epoch 32/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6910 - accuracy: 0.5253\n",
      "Epoch 33/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6911 - accuracy: 0.5246\n",
      "Epoch 34/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6909 - accuracy: 0.5249\n",
      "Epoch 35/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6907 - accuracy: 0.5240\n",
      "Epoch 36/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6908 - accuracy: 0.5266\n",
      "Epoch 37/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6907 - accuracy: 0.5246\n",
      "Epoch 38/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6906 - accuracy: 0.5269\n",
      "Epoch 39/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6906 - accuracy: 0.5275\n",
      "Epoch 40/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6905 - accuracy: 0.5277\n",
      "Epoch 41/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6904 - accuracy: 0.5261\n",
      "Epoch 42/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6904 - accuracy: 0.5278\n",
      "Epoch 43/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6902 - accuracy: 0.5294\n",
      "Epoch 44/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6903 - accuracy: 0.5288\n",
      "Epoch 45/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6902 - accuracy: 0.5309\n",
      "Epoch 46/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6900 - accuracy: 0.5297\n",
      "Epoch 47/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6900 - accuracy: 0.5311\n",
      "Epoch 48/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6899 - accuracy: 0.5316\n",
      "Epoch 49/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6898 - accuracy: 0.5303\n",
      "Epoch 50/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6896 - accuracy: 0.5330\n",
      "Epoch 51/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6897 - accuracy: 0.5314\n",
      "Epoch 52/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6897 - accuracy: 0.5309\n",
      "Epoch 53/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6896 - accuracy: 0.5316\n",
      "Epoch 54/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6894 - accuracy: 0.5333\n",
      "Epoch 55/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6894 - accuracy: 0.5306\n",
      "Epoch 56/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6894 - accuracy: 0.5318\n",
      "Epoch 57/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6893 - accuracy: 0.5328\n",
      "Epoch 58/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6893 - accuracy: 0.5332\n",
      "Epoch 59/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6892 - accuracy: 0.5329\n",
      "Epoch 60/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6892 - accuracy: 0.5325\n",
      "Epoch 61/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6891 - accuracy: 0.5329\n",
      "Epoch 62/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6890 - accuracy: 0.5327\n",
      "Epoch 63/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6889 - accuracy: 0.5336\n",
      "Epoch 64/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6888 - accuracy: 0.5361\n",
      "Epoch 65/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6887 - accuracy: 0.5356\n",
      "Epoch 66/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6888 - accuracy: 0.5349\n",
      "Epoch 67/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6887 - accuracy: 0.5333\n",
      "Epoch 68/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6885 - accuracy: 0.5357\n",
      "Epoch 69/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6885 - accuracy: 0.5351\n",
      "Epoch 70/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6887 - accuracy: 0.5333\n",
      "Epoch 71/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6884 - accuracy: 0.5361\n",
      "Epoch 72/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6884 - accuracy: 0.5364\n",
      "Epoch 73/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6883 - accuracy: 0.5370\n",
      "Epoch 74/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6884 - accuracy: 0.5373\n",
      "Epoch 75/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6883 - accuracy: 0.5360\n",
      "Epoch 76/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6883 - accuracy: 0.5353\n",
      "Epoch 77/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6879 - accuracy: 0.5386\n",
      "Epoch 78/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6881 - accuracy: 0.5371\n",
      "Epoch 79/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6879 - accuracy: 0.5378\n",
      "Epoch 80/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6880 - accuracy: 0.5388\n",
      "Epoch 81/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6879 - accuracy: 0.5385\n",
      "Epoch 82/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6878 - accuracy: 0.5365\n",
      "Epoch 83/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6879 - accuracy: 0.5375\n",
      "Epoch 84/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6878 - accuracy: 0.5378\n",
      "Epoch 85/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6875 - accuracy: 0.5373\n",
      "Epoch 86/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6876 - accuracy: 0.5394\n",
      "Epoch 87/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6876 - accuracy: 0.5395\n",
      "Epoch 88/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6876 - accuracy: 0.5392\n",
      "Epoch 89/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6875 - accuracy: 0.5402\n",
      "Epoch 90/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6874 - accuracy: 0.5396\n",
      "Epoch 91/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6873 - accuracy: 0.5407\n",
      "Epoch 92/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6873 - accuracy: 0.5406\n",
      "Epoch 93/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6874 - accuracy: 0.5388\n",
      "Epoch 94/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6874 - accuracy: 0.5386\n",
      "Epoch 95/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6872 - accuracy: 0.5408\n",
      "Epoch 96/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6871 - accuracy: 0.5419\n",
      "Epoch 97/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6874 - accuracy: 0.5398\n",
      "Epoch 98/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6869 - accuracy: 0.5406\n",
      "Epoch 99/100\n",
      "1250/1250 [==============================] - 2s 2ms/step - loss: 0.6870 - accuracy: 0.5426\n",
      "Epoch 100/100\n",
      "1250/1250 [==============================] - 2s 1ms/step - loss: 0.6871 - accuracy: 0.5410\n",
      "625/625 [==============================] - 1s 1ms/step\n",
      "Accuracy: 0.50\n",
      "Precision: 0.50\n",
      "Recall: 0.52\n",
      "F1 Score: 0.51\n",
      "ROC AUC: 0.50\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "data = pd.read_csv('customer_churn_large_dataset.csv')  # Replace with the path to your preprocessed dataset\n",
    "# Remove columns that are not relevant for modeling\n",
    "data.drop(['CustomerID', 'Name'], axis=1, inplace=True)\n",
    "\n",
    "# Split the data into features (X) and the target variable (y)\n",
    "X = data.drop('Churn', axis=1)\n",
    "y = data['Churn']\n",
    "\n",
    "# Encode Categorical Variables (Label Encoding)\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_cols = ['Gender', 'Location']  \n",
    "for col in categorical_cols:\n",
    "    X[col] = label_encoder.fit_transform(X[col])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature Scaling (Standardization)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Model Building: Neural Network\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=64, verbose=1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "# Model Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# Print model evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aditg\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model to a file\n",
    "model.save('customer_churn_nn_model.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
